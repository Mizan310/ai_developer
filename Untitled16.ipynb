{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL2QKjOn57gt",
        "outputId": "516523f9-341b-42ec-b9fe-e19fdb69d51a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (101400, 36)\n",
            "Training set size: 81120 samples\n",
            "Test set size: 20280 samples\n",
            "\n",
            "============================================================\n",
            "   Training Decision Tree Regressor (From Scratch)\n",
            "============================================================\n",
            "\n",
            "Training...\n",
            "‚úì Training complete!\n",
            "  Tree depth: 10\n",
            "  Number of leaves: 683\n",
            "\n",
            "Making predictions...\n",
            "\n",
            "============================================================\n",
            "   RESULTS: Decision Tree Regressor (Scratch)\n",
            "============================================================\n",
            "MAE  : 0.7966\n",
            "RMSE : 1.0268\n",
            "R¬≤   : 0.4072\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "   Training sklearn Decision Tree for comparison\n",
            "============================================================\n",
            "‚úì Training complete!\n",
            "  Tree depth: 10\n",
            "  Number of leaves: 683\n",
            "\n",
            "============================================================\n",
            "   RESULTS: sklearn Decision Tree (Baseline)\n",
            "============================================================\n",
            "MAE  : 0.7968\n",
            "RMSE : 1.0273\n",
            "R¬≤   : 0.4067\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "   Sample Predictions (First 10)\n",
            "============================================================\n",
            "Actual       Predicted    Error       \n",
            "------------------------------------------------------------\n",
            "7.81         7.48         0.33        \n",
            "7.25         7.17         0.08        \n",
            "7.62         5.70         1.92        \n",
            "7.19         7.17         0.01        \n",
            "8.62         7.79         0.84        \n",
            "6.88         6.72         0.15        \n",
            "6.50         7.96         1.46        \n",
            "8.19         7.69         0.50        \n",
            "7.44         7.64         0.21        \n",
            "9.50         7.94         1.56        \n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "   Top 10 Most Important Features\n",
            "============================================================\n",
            "Rank   Feature                        Importance  \n",
            "------------------------------------------------------------\n",
            "1      WEEKS                          0.816219\n",
            "2      GAINED                         0.035230\n",
            "3      SEX                            0.023056\n",
            "4      RACEMOM                        0.022514\n",
            "5      CIGNUM                         0.022355\n",
            "6      MAGE                           0.016095\n",
            "7      LOUTCOME                       0.006931\n",
            "8      ID                             0.006918\n",
            "9      VISITS                         0.006502\n",
            "10     HYPERPR                        0.005625\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# Decision Tree Regressor (From Scratch) - FIXED\n",
        "# ===============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# ===============================================================\n",
        "# 1Ô∏è‚É£ Decision Tree Node Class\n",
        "# ===============================================================\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature = feature      # Feature index for splitting\n",
        "        self.threshold = threshold  # Threshold value for splitting\n",
        "        self.left = left           # Left child node\n",
        "        self.right = right         # Right child node\n",
        "        self.value = value         # Prediction value (for leaf nodes)\n",
        "\n",
        "# ===============================================================\n",
        "# 2Ô∏è‚É£ Decision Tree Regressor Class\n",
        "# ===============================================================\n",
        "class DecisionTreeRegressor:\n",
        "    def __init__(self, max_depth=10, min_samples_split=2, min_samples_leaf=1, criterion='mse'):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.criterion = criterion\n",
        "        self.root = None\n",
        "        self.n_features = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the decision tree\"\"\"\n",
        "        self.n_features = X.shape[1]\n",
        "        self.root = self._grow_tree(X, y, depth=0)\n",
        "        return self\n",
        "\n",
        "    def _calculate_mse(self, y):\n",
        "        \"\"\"Calculate mean squared error\"\"\"\n",
        "        if len(y) == 0:\n",
        "            return 0\n",
        "        mean = np.mean(y)\n",
        "        return np.mean((y - mean) ** 2)\n",
        "\n",
        "    def _calculate_mae(self, y):\n",
        "        \"\"\"Calculate mean absolute error\"\"\"\n",
        "        if len(y) == 0:\n",
        "            return 0\n",
        "        median = np.median(y)\n",
        "        return np.mean(np.abs(y - median))\n",
        "\n",
        "    def _calculate_criterion(self, y):\n",
        "        \"\"\"Calculate splitting criterion\"\"\"\n",
        "        if self.criterion == 'mse':\n",
        "            return self._calculate_mse(y)\n",
        "        elif self.criterion == 'mae':\n",
        "            return self._calculate_mae(y)\n",
        "        else:\n",
        "            return self._calculate_mse(y)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"Find the best split for a node\"\"\"\n",
        "        best_gain = -float('inf')\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        parent_criterion = self._calculate_criterion(y)\n",
        "        n_samples = len(y)\n",
        "\n",
        "        # Try each feature\n",
        "        for feature in range(self.n_features):\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "\n",
        "            # Try each unique value as threshold\n",
        "            for threshold in thresholds:\n",
        "                # Split data\n",
        "                left_mask = X[:, feature] <= threshold\n",
        "                right_mask = ~left_mask\n",
        "\n",
        "                # Check minimum samples constraint\n",
        "                n_left = np.sum(left_mask)\n",
        "                n_right = np.sum(right_mask)\n",
        "\n",
        "                if n_left < self.min_samples_leaf or n_right < self.min_samples_leaf:\n",
        "                    continue\n",
        "\n",
        "                # Calculate weighted criterion for children\n",
        "                y_left, y_right = y[left_mask], y[right_mask]\n",
        "\n",
        "                left_criterion = self._calculate_criterion(y_left)\n",
        "                right_criterion = self._calculate_criterion(y_right)\n",
        "\n",
        "                weighted_criterion = (n_left / n_samples) * left_criterion + (n_right / n_samples) * right_criterion\n",
        "\n",
        "                # Calculate information gain\n",
        "                gain = parent_criterion - weighted_criterion\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold, best_gain\n",
        "\n",
        "    def _grow_tree(self, X, y, depth):\n",
        "        \"\"\"Recursively grow the decision tree\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Stopping criteria - create leaf node\n",
        "        if (depth >= self.max_depth or\n",
        "            n_samples < self.min_samples_split or\n",
        "            len(np.unique(y)) == 1):\n",
        "            leaf_value = np.mean(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        # Find best split\n",
        "        best_feature, best_threshold, best_gain = self._best_split(X, y)\n",
        "\n",
        "        # If no valid split found, create leaf\n",
        "        if best_feature is None or best_gain <= 0:\n",
        "            leaf_value = np.mean(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        # Split data\n",
        "        left_mask = X[:, best_feature] <= best_threshold\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        # Safety check - ensure both sides have samples\n",
        "        if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
        "            leaf_value = np.mean(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        # Recursively build left and right subtrees\n",
        "        left_child = self._grow_tree(X[left_mask], y[left_mask], depth + 1)\n",
        "        right_child = self._grow_tree(X[right_mask], y[right_mask], depth + 1)\n",
        "\n",
        "        return Node(best_feature, best_threshold, left_child, right_child)\n",
        "\n",
        "    def _predict_sample(self, x, node):\n",
        "        \"\"\"Predict single sample\"\"\"\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._predict_sample(x, node.left)\n",
        "        else:\n",
        "            return self._predict_sample(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict for multiple samples\"\"\"\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            pred = self._predict_sample(x, self.root)\n",
        "            predictions.append(pred)\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def get_depth(self, node=None):\n",
        "        \"\"\"Get the depth of the tree\"\"\"\n",
        "        if node is None:\n",
        "            node = self.root\n",
        "\n",
        "        if node is None or node.value is not None:\n",
        "            return 0\n",
        "\n",
        "        left_depth = self.get_depth(node.left) if node.left else 0\n",
        "        right_depth = self.get_depth(node.right) if node.right else 0\n",
        "\n",
        "        return 1 + max(left_depth, right_depth)\n",
        "\n",
        "    def count_leaves(self, node=None):\n",
        "        \"\"\"Count the number of leaf nodes\"\"\"\n",
        "        if node is None:\n",
        "            node = self.root\n",
        "\n",
        "        if node is None or node.value is not None:\n",
        "            return 1\n",
        "\n",
        "        left_leaves = self.count_leaves(node.left) if node.left else 0\n",
        "        right_leaves = self.count_leaves(node.right) if node.right else 0\n",
        "\n",
        "        return left_leaves + right_leaves\n",
        "\n",
        "# ===============================================================\n",
        "# 3Ô∏è‚É£ Load and preprocess dataset\n",
        "# ===============================================================\n",
        "data = pd.read_csv('Dataset-1.csv')\n",
        "\n",
        "X = data.drop(columns=['BWEIGHT'])\n",
        "y = data['BWEIGHT'].values\n",
        "\n",
        "# Separate numerical and categorical columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Pipelines for preprocessing\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ordinal', OrdinalEncoder())\n",
        "])\n",
        "\n",
        "# Combine transformations\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Apply preprocessing\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "print(f\"Dataset shape: {X_processed.shape}\")\n",
        "\n",
        "# ===============================================================\n",
        "# 4Ô∏è‚É£ Split data\n",
        "# ===============================================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_processed, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
        "\n",
        "# ===============================================================\n",
        "# 5Ô∏è‚É£ Train Decision Tree from Scratch\n",
        "# ===============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"   Training Decision Tree Regressor (From Scratch)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize and train the tree\n",
        "dt = DecisionTreeRegressor(\n",
        "    max_depth=10,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    criterion='mse'\n",
        ")\n",
        "\n",
        "print(\"\\nTraining...\")\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "print(f\"‚úì Training complete!\")\n",
        "print(f\"  Tree depth: {dt.get_depth()}\")\n",
        "print(f\"  Number of leaves: {dt.count_leaves()}\")\n",
        "\n",
        "# ===============================================================\n",
        "# 6Ô∏è‚É£ Make predictions\n",
        "# ===============================================================\n",
        "print(\"\\nMaking predictions...\")\n",
        "y_pred = dt.predict(X_test)\n",
        "\n",
        "# ===============================================================\n",
        "# 7Ô∏è‚É£ Performance evaluation\n",
        "# ===============================================================\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"   RESULTS: Decision Tree Regressor (Scratch)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"MAE  : {mae:.4f}\")\n",
        "print(f\"RMSE : {rmse:.4f}\")\n",
        "print(f\"R¬≤   : {r2:.4f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===============================================================\n",
        "# 8Ô∏è‚É£ Compare with sklearn's Decision Tree\n",
        "# ===============================================================\n",
        "from sklearn.tree import DecisionTreeRegressor as SklearnDT\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"   Training sklearn Decision Tree for comparison\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sklearn_dt = SklearnDT(\n",
        "    max_depth=10,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "sklearn_dt.fit(X_train, y_train)\n",
        "y_pred_sklearn = sklearn_dt.predict(X_test)\n",
        "\n",
        "print(f\"‚úì Training complete!\")\n",
        "print(f\"  Tree depth: {sklearn_dt.get_depth()}\")\n",
        "print(f\"  Number of leaves: {sklearn_dt.get_n_leaves()}\")\n",
        "\n",
        "mae_sklearn = mean_absolute_error(y_test, y_pred_sklearn)\n",
        "rmse_sklearn = np.sqrt(mean_squared_error(y_test, y_pred_sklearn))\n",
        "r2_sklearn = r2_score(y_test, y_pred_sklearn)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"   RESULTS: sklearn Decision Tree (Baseline)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"MAE  : {mae_sklearn:.4f}\")\n",
        "print(f\"RMSE : {rmse_sklearn:.4f}\")\n",
        "print(f\"R¬≤   : {r2_sklearn:.4f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===============================================================\n",
        "# 9Ô∏è‚É£ Sample predictions\n",
        "# ===============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"   Sample Predictions (First 10)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Actual':<12} {'Predicted':<12} {'Error':<12}\")\n",
        "print(\"-\" * 60)\n",
        "for i in range(min(10, len(y_test))):\n",
        "    error = abs(y_test[i] - y_pred[i])\n",
        "    print(f\"{y_test[i]:<12.2f} {y_pred[i]:<12.2f} {error:<12.2f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ===============================================================\n",
        "# üîü Feature Importance (using sklearn model)\n",
        "# ===============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"   Top 10 Most Important Features\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "feature_importance = sklearn_dt.feature_importances_\n",
        "feature_names = list(numeric_features) + list(categorical_features)\n",
        "\n",
        "# Get top 10 features\n",
        "top_indices = np.argsort(feature_importance)[-10:][::-1]\n",
        "print(f\"{'Rank':<6} {'Feature':<30} {'Importance':<12}\")\n",
        "print(\"-\" * 60)\n",
        "for rank, idx in enumerate(top_indices, 1):\n",
        "    print(f\"{rank:<6} {feature_names[idx]:<30} {feature_importance[idx]:.6f}\")\n",
        "print(\"=\"*60)"
      ]
    }
  ]
}